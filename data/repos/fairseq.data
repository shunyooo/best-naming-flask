https://github.com/pytorch/fairseq,pytorch/fairseq,5203,1131
content_length
orig_stdin
idx_size
rgs_mock.optimizer_overrides
raining
negs
smooth_crit
state_dict['loss_scale']
src_tokens_no_pad
prefix
ignore_results
ropout_in
nll_crit
hypo_tokens[i]
segments
sentence_loss_sum
kwargs['data']
inal_layer_norm
bitext2_score
shard_ids
arget_lengths
symal_bin
line_id
updates
rerank_args
new_indices
rc_tokens
hort_seq_prob
tgt_size
pe_ranks
ctx.padding_l
underlying_criterion
ref
parsed_samples
grads_group
args.no_decoder_final_norm
encoder_out["encoder_padding_mask"]
indices
max_tokens
label_dict
max_sample_len
num_toks_in_doc
equence_generators
last_included
lr_shrink
random_doc
ptx
esidual_proj
second
ate1
ate2
final_return
ctivation_dropout
n
banned_tokens[bbsz_idx]]
pred_pos_score
sample_idx
temp_file_path
exclude_patterns
preds
cpu_buffer[0]
metadata
ated
score_dict[id_num]
ymbols
cand_mask
aligned_feats
sample_json
cur_np
ask_whole_words
group_archs
eval_lm_args
path
torch.Tensor
hypo[i]
encoder_a
args.adaptive_softmax_dropout
eval_lm_parser
result['cpc_logits']
items
k
ypo_fracs
num_bpe_tokens_dict[key]
sample[k]
sent_b
length_no_eos
version_key
sample_key
projection
question_toks
kv_same_dim
target_len
rev_tokens
ex_vals
np_start
MODEL_REGISTRY[name]
txt
ransformer_sampling
dec_lines
master_doc
ds_remaining
result['cpc_targets']
scale_window
ask_index
m1
subkey
m3
m2
args.dropout_features
type
args.decoder_freeze_embed
src_bin
left_context]
qdm
gt_bos
odels
ut_channels
pos_num
bw_dict
ata_file
word
mm
os_score
buffered_params
pt_regexp_update_based
ncoder_decoder_attention
uery_size
example
bpe_tok
args.share_encoders
nll_logging_output
new_item[rand_mask]
stats['wall']
max_lengths
args.residual_scale
cache_dir
ntokens
state['_server_tmp']
elf_attn_layer_norm
enc
end
threshold_nwords
args.tgt_embed_dim
feature
next_sent_label
writers
lang_pair_keys
gate
config
n_proj_weight
truncated_probs
r_tsz
target_b_length
after
sampling_factor
largest_j
itr_state
inner_states
no_bpe_sen
r_scheduler.best
gen_param
args.activation_fn
rgs_mock.reset_meters
input_feed
f_dim
dataloader
src_tokens_i
args.dropout
src_tokens_t
eature_index
entence_encoder
tgt_len
fairseq_rel_path
order
current_length
prev_norms
os.environ['PYTHONWARNINGS']
ample_distance
val_lang_pairs
'fairseq-eval-lm
'BLEU{}
before
generate_args
num_words
asking_ratio
checkpoint_conds['checkpoint_best.pt']
exp_avg_sq_row
kwargs
sample['id']
cls
padding_idx
escore_pos_score[index]
ps_mode
eters['oom']
bitext1
bitext2
in_sample_size
needs_sphinx
atasets[split]
iversity_strength
logging_output_keys
args.vggblock_enc_config
lang_pair_dataset
skipped_toks
pretrained_encoder_embed
dummy_batch
emb_dim
INCREMENTAL_STATE_INSTANCE_ID
ax_positions
bpe_status
backtranslation_batch_result
yte_encoder
mean
DEFAULT_ENC_TRANSFORMER_CONFIG
new_indices[symbol]
crit
args.conv_aggregator_layers
grads_are_scaled
s3_path
stats['wps']
onv_bias
dst
checked_src
st:]
saved_groups
prefetch_idx
lprobs.batch_first
sample[j]
log_probs[i]
new_model
um_experts
s_bidirectional
um_attention_layers
decoder_outs[key]
args.skip_connections_feat
prev_cells
_model_args
banned_tokens
state['optimizer_history'][-1]['num_updates']
slice_indices
ata
topk_filled_outputs
messages
ath
weight_grad
beam_size
onvolutions
log_output
angs
trained_encoder
prev_cells[i]
files_to_desymlink
rand
im_offsets
cand_lprobs
rank
sen_scores[line_id]
force
globals()[_model_type]
args.agg_zero_pad
args.max_tokens_valid
sorted_indices
tok
remaining_refs
og_interval
cycle
max_update
B
eters['loss_scale']
group['lr']
tokens
ud_paths
ut_file
source_lengths
angs2id
target
warmup
ampling_topk
hypothesis_dict[j]
project
ampling_topp
armup_rate
utput_proj
encoders[lang_pair]
buffers[name]
future_to_sample
args.share_decoders
maxT
target_prefix_len
seed
seen
caler
m_dict
params_avg
ource
optim_history
input_embedding
fname
output["token"]
word_embs[pads]
selected_samples
ypo
max_positions
mbed_dim
mask_whole_words
optimizer
bsz
istributed_world_size
fp32_params
lprobs
max_pos
chunk_size
todo_include_todos
shared_decoder
max_mem
doc
group_args
bwd_align_file
args.decoder_input_dim
log_probs
R
override_parser
pace_tok
test_sents
cand_toks
log_outputs
adding_value
oom_batch
wrapped_model
egment_id
out_p
doc_mask
label_path
pos_score_dict[j]
end_of_epoch
probs
ictionary.eos()]
cand_scores
flat_words
args.adaptive_input
args.warmup_init_lr
num_heads
exp_inf
in_top2_prob
ource_lengths
ert_tokenizer
dst[1:]
nput_size
utput_dictionary
selected_key
label)
agg_logging_outputs['posterior']
finalized[sent]
github_doc_root
next_epoch_itr
bar
nonzero_buffer
emb
gen_output_lst
weight_expanded
release
module_path
bleu
no_bpe_sentences
node[k]
pads
num
w3
w2
w1
data_samples
args.distributed_rank
n_proj_bias
ormalize_before
best
batch_first
prev_hiddens
tensor
extra_flags
filename
hypo_prefix
score
um_mel_bins
olerance
C
ropout_feats
embeddings
pe_symbol
pretrained_encoder_embed.weight.requires_grad
state_dict[subkey]
ecoder
num_toks
0]
start_idxs[i]
features_only
abel_dictionary
answers
ooms
nclude_targets
peakers
bias_correction1
half_dim
args.share_decoder_input_output_embed
ddp_class
embed_dict[pieces[0]]
high_sampling_topp
candidate_masks
unk
mb_layer_norm
lm_res_lst
bbsz
output_text_file
seqlen
align_dict[srcidx]
cpu_dict[k]
num_remaining_sent
lang_encoders[lang]
grad_tbc
diff
extra_args_setter
bsz_mult
concat_h
mbeddings
new_dico
frm
trainer.get_num_updates.return_value
init_kwargs['check_reduction']
beta2
C2
C1
odel_fname
orld_size
:beam_size][blacklist]
cpu_dict
nnodes
state['args'].max_source_positions
acktranslators[lang_pair]
cumsum_probs
end_idx
target_idxs
exp_avg
generate_parser
ax_lr
init_kwargs['find_unused_parameters']
modified_lengths
ormalize_scores
sample_size
checkpoint_index
labels[utt_id]
src_lengths
cand
pos_scores
niform_prior
fp16_optimizer
ut_dim
target_seq_length
vocab_keys
id
weights[:self.vocab.nspecial]
state_dict['{}.embed_positions._float_tensor'.format(name)]
pos_score[i]
forward_input
plit
valid_losses
output_dim
ord_shuffle
encoder_padding_mask
args.encoder_bidirectional
split
args.sent_loss
total_len
sent_a
num_bpe_tokens_dict[line_id]
left_pad_input
refix
dataset_idx
retrained_decoder
per_channel_dim
tgt_datasets
prob_z_xy
rain
args.bias_kv
lang_decoders
word_to_char
nit
averaged_params
n_projection
eps
eters['gnorm']
batch_sampler
client
rev_output_tokens
nput_feeding
smooth_loss
src_item
files_to_delete
component_state_dict
sentence
t_curr
score_sum
extra_state['train_meters']
collated_sources
p.requires_grad
noising_datasets
ew_src_eos
adding
ep_word
new_dict
onv_dim
target_size
plasma
query_span
ordered_targets
a_list
fast_align_bin
DEFAULT_MAX_TARGET_POSITIONS
args.sentence_avg
:self.num_words]
fusion
mbed_scale
score1_file
langpair_dataset
step]
args.dataset_impl
d
rerank1_is_gen
ex_len
args.buffer_size
bpe
final_cells)
abel_index
args.decoder_output_dim
steps
eturn_masked_tokens
neg_idxs
fwd_align_file
args.beam_probs
eft_pad_target
truncated
pad_mask
cur_data
chunks
hypos
shared_decoder_embed_tokens
write_targets
rray
manifest
module_instance._fairseq_instance_id
pretrained_decoder_embed.weight.requires_grad
total
huffle
current_epoch_iterator
escore_target
encoded_lines
overrides
ord_dropout
ntasks_per_node
core
args.apply_bert_init
ccumulate_grads
args.target_lang
s["lm_target"]
target_lengths
has_target
output_embed_dim
corr
intersphinx_mapping
tgt_file_name
masked_index
nable_torch_version
item['source']
TOKENIZER_REGISTRY
gradInput
sample_seq_len
eight
masked_sent[i]
REGISTRY
eature_extractor
bpe_data
args.cross_sample_negatives
float
en_penalty
ncoder_cell_proj
sample_len
tgt_eos
abel
args.criterion
acktranslation_fn
frameinfo
atch_source_len
state_dict['last_optimizer_state']
tgt_item
ayers
component_subkey
otal_num_update
args.sentence_class_num
rozen_batches
state['exp_inf']
in_d
num_models
head
prev_grad_norm
chunks_reset
encoder_embed_tokens
feature_aggregator
state['_plasma']
encoder
gelu_accurate._a
rand_mask
decoder
entence_projection_layer
grad_data
self_attn_state
eters['clip']
concat_rest
moothed_grads_localprev
keys_to_delete
attn
vocab_file_name
train_args
args.left_pad_target
sample_ratios[0]
mbedding_dim
item_len
flat
argets
model1
id_num
args1.probs
gradFilters
in_len_a
id_map
numHeads
p16_params
in_channels
rerank_args['gen_subset']
write_hypos
input_buffer
nk
preprocess_args
input1d
minibatch
actor
element_sizes
query_with_ws
args.pooler_dropout
denoising_batch_result
WEIGHTS_NAME
node
left_pad
DEFAULT_ENCODER_JSON
args.pretrained_checkpoint
res_files
detok
src_lengths[i]
update
ample_ratios
ntok
eos_prob
scorer
scores
utt_id
sent_indices
encoders
REGISTRY[name]
layer_norm_map
bwd_fast_align_cmd
Optional[int]
embed_dim
amsgrad
gt_dict
layers
spacy_nlp._nlp
trained_decoder
os]
criterion_name
models_dir
args.prediction_steps
chunk_input
last_optim
args.device_id
nbest_file_id
args.share_all_embeddings
num_group
Optional[torch.Tensor]
args.max_sentences_valid
choice
masked_token
lrs
entries
tgt[i]
mask_start
dim
lm_loss
start_idxs
url_bytes
param_scale
iter
prob
p
defaults
lm_logits
lprobs_nopad
args.encoder_layers
prev_inner_dim
max_len
ids[i]
decoder_lang_tok_idx
new_toks[i]
targets
2s_collater
slice_e
DEFAULT_DEC_TRANSFORMER_CONFIG
param_group['lr']
builtin_print
seqs
arget
html_context
file_path
g_tbmf_wrapper
token_length
postfix['epoch']
args.encoder_freeze_embed
min_step
ransformer_context
ax_char_len
tart_time
valid_log
groups
address
langs
qa_file_paths
target2
target1
write_timer
ARCH_CONFIG_REGISTRY
query_masks
encoder_out[key]
grad_shape
unmask_prob
frames1
frames2
cand_spans
max_dim
CHAR_EOS_IDX
ambda_otf_bt
extra
lenpen
dtype
initial_params[i]
transformer_input_dim
residual
top_indices
conv_op
word_embs
frames_lengths
char_idxs
tail_priors
rand_id
pred
roject_input
predictions
rc_dict
msg
egment_embeddings
prev
ncoder_attn_layer_norm
args.decoder_embed_dim
src_str_with_unk
outputs
o_bpe_sentences
ut_projection
encoder_outs
std
head_name
len_to_consume
hreshold
tgtidx
matching_files
grad_norm
freq_map
ecay_factor
smoothed_grad
start_ds_idx
new_blacklist
input2
input0
input1
ackwards
se_position_embeddings
stream
:]
decoders
dataset_map[lang]
sample
outs
p_data_fp32
kip_connections
final_cells
1
uffer
criterion
cur_dir
CRITERION_REGISTRY
min_lens
inputs
sent_length
eights
query_lengths
flat_logging_output['nll_loss']
b]
data
all_gather_list._cpu_buffer
sz
modified_x
alanced_classes
weight_grad_short
trainer.load_checkpoint.return_value
sr
sp
rame_shift
st
q
switch
smooth_logging_output
sm
c3.weight
bpe_cont_marker
lr_new
lign_dict
um_heads
newly_finished
cur_path
rerank_args['weight1']
checkpoint_conds['checkpoint{}.pt'.format(epoch)]
cols
512*2048*2
eps_i
first_doc[i]
source_dict[j]
predicted_idx
group
jin
masked_tokens
args.no_token_positional_embeddings
TASK_REGISTRY
lang2id
prefix_mask
hypo_prefix_len
:self.eos]
args.activation_dropout
ranspose
lm_res
src_delta
riterion
bucket_name
seq_lengths
updated_value
correct_sum
n_proj_k
name
args.weight_dropout
utoff
reference_lst
fusion_output
512*5000
leading_space
n_proj_q
ptimizer.state[param]
stats['oom']
logging_output
en
ei
decoder_out
score_dict
old_accumulate_grads
ef
ang_pairs
args.adaptive_input_factor
outprefix
pos_score
summary
in_length
escore_target[index]
C2.bias
DEFAULT_DEC_CONV_CONFIG
steps_in_stage
poch
bpe_merges
lock_lr
correct
rediction_steps
ref_ids1
ref_ids2
src_tokens[b][predicted_idx]
args.transformer_dec_config
r_shrink
args
512*512
input_embed_dim
dependencies
args.decoder_dropout_in
last_optim_state
weights_key
init_lr
ord
remainder
acktranslators
noising_gen
nput_root
training
forward_input["src_tokens"]
language
char_embs
src_langs
bject_id
encoder_json
flat_embeddings
len_max
conv_result
first
valid_res_log
variables
1]
sentence_logits
model_name_or_path
selfattention
target_embedding
rescore_file
srclen
input_buffer[k]
onv
attn_clone
size
eed_attn
um_hypos
ache[token]
nk_index
fmt
padding_l
trained_encoder_out
2
params
padding_r
set_type_c
onv_layer_norm
eters['valid_nll_loss']
ias
args.max_sentences
v_layers
args.gru_dim
args.adaptive_input_cutoff
templates_path
query_lprobs
blacklist
m)]
x2
out_ds["features"]
last_overflow_iter
is_target_list
tuneable_parameters
eplace_map
qa_files
prev_output_tokens_k
r
param1
module_name
finished[sent]
rerank2_is_gen
dummy_src_samples
query_mask
word_embs[unk]
speakers
source_suffix
lprobs_g
tgt_dict
ync_iter
ncoder_output_dim
ordered_indices
um_conv_layers
src_eos
word_lens
realpath
begin
buffer
score_dict[j]
acktranslate_datasets
start_offset
og_prob
src_dataset
escore_pos_score
sharded_len
net_outputs
encoder_out["encoder_out"]
extra_meters
]
pe_tokens
len(tgt[i])]
D)
ummaryWriter
eters['bsz']
dir_path
emperature
emove_eos_from_src
lock_momentum
embed_dict
expected_tgt
um_vggblocks
ount[idx]
rocess_group
nlp
scores_g
flat_logging_output['sample_size']
args.decoder_layers
path_1
ns2
rand_doc_id
input_lengths
riterion_cls
dict
init_kwargs
Translation
start_rank
args.tie_adaptive_proj
ord_blanking_prob
state['exp_avg_sq_col']
full_hypo
H
tail_out
ayer_norm
bpe_tgt_param
highlight_language
progress
score_dict[key]
grad_weights
agg_logging_outputs
new_state['model']
args.in_channels
ail
end_cut_a
old_steps
dataset_impl_k
ignored
hyp_pieces
attn_weights
default_cache_path
retrained_encoder
pt_regexp_epoch_based
state['args'].task
word_shuffle
utput_collater
l_noised
input["length_ms"]
__all__
agg_output["nll_loss"]
path_0
label
word_stats
etag_hash
outf_label_path
ndices
entence_out_dim
tune_parameters
agg_loss
args.aggregator
ross_sample_negatives
model_specific_group
new_bsz
src_lengths[0]
args.add_bos_token
lm_targets
ocab_size
s
stats['ups']
path_k
embedding
net_input
agg_output
noisy_src_lengths
stdin_bak
targets_two
izes
pygments_style
pos
score[i]
line_type
ataset_1
args.encoder_attention_heads
ataset_2
node[full_k[-1]]
agg_output['nll_loss']
param.grad
bpe_tokens
utput_embed_dim
ocab
ncoder_output_units
odel
pretrained_decoder_embed
store_data
rojection
s_bpe
args.share_decoder_embeddings
sort_key
inear2
inear1
prefix_len_rescore_file
mbedding
x_len
itr_pos
ambda_parallel
new_s
rgs_mock.reset_dataloader
state['optimizer_history'][-1]['lr_scheduler_state']
numFiltersInBlock
escore_source[index]
magic_test
update_freq
vocab_size
rerank_args['weight3']
new_state
wrapped_criterion
stdout
word_begins_idx
supports_prefetch
tgt_datasets[lang_pair]
pooled_output
meta_path
idxs
bias
mask_size]
eriod
args.tokens_per_sample
pe_end
author
ncoders
new_order
epoch
pad
escore_source
lassif_token_idx
batch_mask
esidual_scale
extended
ax_seq_len
erbose
roject_features
eters['valid_loss']
group_task
elf_attn
save_checkpoint.best
html_static_path
is_update_based
exp_avg_sq
adding_l
r_scheduler
rref[rref.eq(self.unk)]
new_word
validate_parser
srcidx
utts
args.lenpen
encoder_cells
absolute_max
source[i]
ata_offsets
sample_rate
orward_output
args.inputs
escore_hypo[index]
checkpoints
rc_dataset
args.decoder_embed_path
resp
batch['net_input']['prev_output_tokens']
rest
subset_summaries
av2vec_predictions
transformer_layer_idx
agg_dim
ncoder.num_attention_layers
ppend_eos_to_target
score_lst
eters['train_nll_loss']
1:]
generator
param
rame_sizes
samples
has_eos
momentum
offsets[i]
gate2
gate1
MODEL_REGISTRY
rediction_h
trim_mask
eading_space
backwards_preprocessed_dir
target_tokens
embs
lm_res1
gen_tokens
mod
freq_map[srcidx]
loo_bleus
p32_optimizer
outf_context
server
first_doc
t
task_name
sample2
args.no_conv_bias
output
lm_bpe_tokens
sequenceLength
bpe_cont
lr_correct
lm_score_file
max_shuffle_distance
next_sent
spacy_tokenizer._tokenizer
args.enc_output_dim
args.encoder_normalize_before
data_itr
frame_sizes
averaged_params[k]
bitext2_norm
ndices[word]
oss_scale
ordered_hypos
emove_eos_from_source
new_tgt_bos
ndices_buf
flat_logging_output['loss']
unmask
max_exp_avg_sq
subset_one
ictionary.eos():]
set_type
in_lr
new_toks
bin_buffer_mmap
bitext_score2
avg_probs_i
_
iversity_buf
src
params_dict
ok
_stdin_lock
tepsize
_hypo
step_size
weighted_features
os
ad_idx
x[i][j]
escore_hypo
eos_mask
ew_tgt_bos
hypo_str
filterSize
idx2
new_src_eos
new_target[0][mask]
eak_lr
lean_up_tokenization
PYTORCH_FAIRSEQ_CACHE
params_dict[k]
start
src_len
out_proj_size
args.max_positions
head_sz
_fields_
hunk_size
ort_order
eos_mask_batch_dim
ordered_hypos[gen_keys[key]]
um_hypos[index]
is_bpe
entences
datasets
search_text
state['last_optimizer_state']
ord_tokenize
default
ongest_dataset_key
embedder
data_name_or_path
sample['start_indices']
tied_proj
embed
sen_pos_scores[line_id]
pretrained
REGISTRIES
eos_idx
min_block
ad
ai
al
subset_two
oad_softmax
new_dict[i]
file
group['lr_old']
collated_sources[i]
args.balanced_classes
predicted_token
past_target
gated_x2
ref_tok
reload_ids
args.encoder_kernel_size_list
args.downsample
denom
args.attention_dropout
cached_state
input_file
symbol
args.act_dropout
set_params
args.share_input_output_embed
et_word_idx
pool
ize
args.adaptive_softmax_cutoff
nsure_first_token
mask
source_dict
xlm_loaded_state_dict
param_state['momentum_buffer']
shuffle_map
asked_lm_pooler
spacy_toks
um_segments
src[0]
inf_scores
is_generation_fast
new_list
attrs
source_dict[id_num]
REGISTRY_CLASS_NAMES
sample_ratios
ord_to_char
ead_index
batches
eos_scores
min_lr
tr
src_datasets
batched
DEFAULT_MAX_SOURCE_POSITIONS
ti
roject_in_dim
onvolutions[i]
tokens_ds1
tokens_ds2
input1d.requires_grad
sym_out_file
rc
translations
in_len_b
results
ls_word
ignore_grad
word_embs[eos]
eft_pad_source
args.decoder_glu
ad_right
s["source"]
d_p
nk_penalty
ollate_fn
output["tokenid"]
ls_index
noun_chunks
param.requires_grad
mbed_positions
max_num_tokens
masked_tgt_one
sample_sizes
repend_getter
args.decoder_dropout_out
tokens_ds
input_option
stats['num_updates']
prefix_toks
version
tgt_dataset1
tgt_dataset2
scores_buf
option2
eal_sizes
bpe_args
stats['train_wall']
K
ad_index
ranges
caler.loss_scale
next_lr
args.encoder_embed_dim
ask_word
dicts
collater
base
permutation
eval_lm_param
bitext1_norm
options
predictions[start:end]
trainer
pplied_patches
shuffle
tokenizer
state['exp_avg_sq']
lang2id[lang]
args.decoder_normalize_before
args.adaptive_softmax_factor
hypo_tokens
html_theme
epoch_itr
block_size
oftmax_batch
ambda_otf_bt_steps
linearized_weight
retrained_outputs["out"]
bpe_counts
ord_proj
masked_tokens[mask]
acrebleu
state_size
ncoder_input
eys
conv_feature_layers
MILLISECONDS_TO_SECONDS
logprob
forward_input["prev_output_tokens"]
args.max_decoder_positions
align_dict
preprocess_lm_param
other_tok
last_rescale_iter
score_i
proj_x
o_repeat_ngram_size
gen_timer
d.unk.return_value
num_embeddings
data_file
suppressed_parser
dev
ense
rc_eos
v
args.zero_attn
registry_name
filtered_chunks
saved_criterion
bitext2_lst
shard
avg_attn
t[-1]
args.encoder_ffn_embed_dim
logits
sents_seen
rel_step_sz
left_to_right_preprocessed_dir
args.project_input
ppend_eos
args.input_feat_per_channel
prev_hiddens[i]
len_tgt
hyp_words
noising_dataset
saved_state
target_str
um_words:]
ep_index
args.project_features
lang_decoders[lang]
subsampling_factor
grad_logp
gen_parser
okens_per_sample
adding_idx
ax_sample_size
response
a
truncated_indices
P
postfix
s3_object
ooling_kernel_size
ax_source_positions
spacy_toks_ws
flat_logging_output['ntokens']
finalized
data_dict
current_head_names
__builtins__.__NUMPY_SETUP__
ad_left
end_cut_b
DEFAULT_TEST_VOCAB_SIZE
bpe_toks
lprobs[prefix_mask]
etok
reorder_state
args1
sys_tok
itr
features
args.dropout_agg
state_dict['criterion']
bpe_src_param
grad1d
LR_SCHEDULER_REGISTRY
args.distributed_no_spawn
ncoder
pre_gen
efault_dropout_prob
align_dict[cols[0]]
tokens_one
using_nbest
targets_one
out_buffer
2]
beams_buf
front_cut_b
L
rerank_args['lenpen']
front_cut_a
magic
ys
tgt_probs
__version__
ambda_parallel_steps
dd_eos_for_other_targets
_attn_weight
token
new_target
found
decoder_embed_dict
preprocess_dir
lprob_yz
weight
sync_para
hare_input_output_embed
buffers
stats['ppl']
freq_map[srcidx][tgtidx]
utput_root
pt_regexp
stats['gnorm']
item
nit_lr
ctual_size
ictionary
etag
fused_adam_cuda
num_unused_segments
best_hypo
sentences[line_id]
cpu_buffer[1]
lower_chunk
adaptive_softmax
offsets
final_else
'fairseq-train
args.conv_feature_layers
lr_old
o_bpe_source
ask
buffer_size
nloss
rads_localprev
pairs
tgt_langs
sentence_rep
first_beam
extra_state
incremental_state[full_key]
ensemble
prev_num_classes
atches
model_path
w
curr_prob
se_feat
generated_src
ARCH_MODEL_REGISTRY[arch_name]
prev_char
'fairseq-generate
avg_attn_i
word_begins_mask
ropout_out
ownsample
future_mask
nput_proj
full_attn
checkpoint
args.sample_distance
_dict__
target_lengths[i]
eft_pad
stats['clip']
terable
tats
saved_state['prev_key']
truncated_sent_b
src_file_name
truncated_sent_a
option
sym_cmd
lm_score
bidirectional_sparse_mask
ppend_eos_to_tgt
bpe_len
REGISTRIES[registry_name]
state['exp_avg']
gen_keys
start_index
split_path
decoders[lang_pair]
stm
dst[0]
gt_vocab
str
bw_rescore_file
args.threshold
trailing_space
embed_keys
tride
bpe_parser
CHAR_PAD_IDX
args.self_attention
rgs.sentence_avg
sort_order
logging_output['loss']
sentences
word_prob
issing_next_words
dtype_code,
paths
encoder_embed_dict
armup_updates
new_symbols
um_layers
ambda_denoising
random_start
k2
k1
efn
um_workers
cand_text
args.encoder_hidden_size
elu_dropout
w2_prob
target_dict[id_num]
ctx.scale
chars
override_args
node_id
noise[0]
bpe_count
feature_enc_layers
_task
ncoder_hidden_proj
model
k]
attention_sparse_mask
combined_scale
extra_link_args
model_params_keys
input[i][0]
state_dict[version_key]
ks
ext
sample['target']
kw
'fairseq-interactive
src_tokens[b][t][predicted_idx]
rerank_scores
dd_bos_token
main_block
ncremental_states
bin_buffer
ropout
expected_scores
checkpoint_path
copyright
checkpoint_conds['checkpoint_last.pt']
decoder_out[0]
writer
etag_bytes
hard_id
factor
output_dictionary
vocab
model.gating_network
sorted_langs
target_dict
Batch
acktranslate_datasets[lang_pair]
har_embeddings
shared_dict
nll_loss
bpe_end
bw_preprocess_param
num_bpe_tokensn
xtension
elfattention
_negatives
bytes
dropout_prob
args.input_dropout
hypo
avg_probs
ncoder_attn
x
ask_prob
right_to_left_preprocessed_dir
eight_softmax
gen_model2_param
seq
sorted_samples
num_sentences
target_prefix
ctivation
okenizer
uffer_size
everse_order
expert
lang_part
readme
lassification_heads[name]
ropout_agg
data_utils_fast
args.warmup_updates
encoder_dim
nitial_state
attention
batch_size
utput_units
output_tbc
numel
source_prefix_frac_rescore_file
args.character_embeddings
stats[key]
c
dataset_index
prev_best
component_type
roject_to_steps
tokens_2
tokens_1
data_json_path
railing_space
args.share_encoder_input_output_embed
enc_size
r_step
context
train_meter
collection
cand_masks
pdb
escore_score[index]
rojections
tgt
rrors
wps_meter
sent_id
um_shards
ax_len_b
gradWeight
ax_len_a
pct_overflow
ids[nwords]
ransformer_layers
kernels
num_docs
pu
args.gated_attention
'fairseq-preprocess
ttention_module
sample_bsz
sorted_order
pe
scorer1_src
src[i]
masked_tgt_two
right_context
cached_result
dropout
param_values
args.share_encoder_embeddings
(C
ource_lengths[index]
query_len
batch
convs
num_generated_tokens
replaced
dummy_encoder_output
tgt_words
cand_indices
shortened
DEFAULT_VOCAB_BPE
target_padding_mask
len_a
len_b
rin
src_token
ower
um_groups
batch_mask[cand_indices.new(finalized_sents)]
eight_linear
minim
lengths
stats[k]
node_list
port
dest
cand_offsets
ormalize
educe
embedding_shape
args.decoder_attention
ext_word_prob
denoising_lang_pairs
left_context
cleaned_text
ncorrect
levels
expected_src
ample_rate
et_out
ttproj
args.log_format
task
r_scheduler.last_epoch
oot_dir
pad_idx
args.transformer_enc_config
idden_size
cur_epoch_itr
con_else
y
cores_buf
full_path
T
use_first_moment
ache_index[i]
questions
head_y
world_size
alignment
eams_buf
rgs
num_updates
scorer2_src
rapped_optimizer
nwords
stats['wpb']
pply_bert_init
source
bidirectional_attention_sparse_mask
input
attn_buf
rand_or_unmask
encoder_b
net_output
avg_nll_loss
output1d
mask_num
start_id
dtypes
other_tokens
shared_encoder
break_mode
num_bpe_tokens
trained_model
fw_rescore_file
args.fp16
rounded_index
found_tok
use_cuda
tensor[mask]
eed
args.multihead_self_attention_nheads
s["segment_labels"]
args.max_tokens
last_return
ambda_denoising_steps
earch
masked_sent
lr_range
B)
args.pooler_activation_fn
examples
lass_proj
elu
args.left_pad_source
x_unfold
t_i
ength
total_size
cale_factor
decision
tgt_delta
output["text"]
src_tokens
included_word_indices
proj
text_spans_bpe
sample1
orward_input
sample_from_first_ds_percentage
oining
pronoun_index
checked_tgt
args.encoder_attention
step
negatives
bs
DEFAULT_ENC_VGGBLOCK_CONFIG
ache
lock_indices
input_tbc
agg_logging_output[logging_output_key]
args.max_source_positions
atasets
as_pairs
idx
rc_sizes
ids
ndex
range
extensions
packed_x
ARCH_CONFIG_REGISTRY[arch_name]
bleup
nput_dim
s3_resource
cutoff
fp32_optimizer
source_lengths[i]
ut_proj
warmup_end_lr
esiduals
inp_sz
frames
TASK_CLASS_NAMES
orig
span
args.max_target_positions
query_tokens
cale_window
question
nframes
state['_server']
args.decoder_attention_heads
onv_kernel_size
cur_doc
final_hiddens
int]
d.pad.return_value
arget_lengths[index]
files
batch_idxs
new_tgt
kb
url_hash
ep_token_idx
sequence_if
hypothesis_dict[id_num]
st:en]
right_pad_input
delta
ns1
line
new_line
retrained_outputs
lprob_y
trainer.get_train_iterator
sent_sizes
tgt_tokens
eam_size
um
scorer2_tgt
z
sym_fast_align_bin
right_pad
curr
_stdin
curr_sample_rate
'fairseq-validate
m
parser
state_dict_subset
args.transformer_context
tempSumGradFilters
n_channels
arams_localprev
low_sampling_topp
parsed
Sample
ead_dim
codes
topk_predicted_token_bpe
bitext_bpe
inner_dim
args.num_negatives
tempdir
target[b][0]
tart
padding
ighway
next_prob
#html_sidebars
resolved_archive_file
hostnames
current_chunk
extra_compile_args
bad_filter
rgs_mock
ataset
segments_two
e
econstruct_idx
TASK_REGISTRY[name]
agg
cand_beams
scorer1_tgt
probs[idx:end]
cache_path
armup_iteration
weights
at
hypothesis_dict
t_src]
gt
OPTIMIZER_REGISTRY
eters['train_wall']
layer_norm
best_function
len_src
prev_value
query
updates)]
n_seq_tok
gen_model1_param
src_dict
num_bpe_tokens_dict
_class__
src_str
loss_sum
args.num_segment
sent
args.skip_connections_agg
stats['lr']
probs_nopad
labels
ppend_eos_to_src
args.moses_source_lang
outf_context_path
flat_logging_output
args.decoder_hidden_size
total_param_size
state['RMS']
doc.user_token_hooks['vector']
num_cpu
input_size
target_dict[j]
bbsz_offsets
runcation_length
positions
bias_correction
conv_tbc
aud_paths
earned_pos_embedding
stats['loss_scale']
prefix_tokens
token_block_utils
nd_learning_rate
pronoun_idx
args.weight3
sent_size
args.weight2
gen_args.beam
weight_decay
tmpdir
index
rgs.probs
rc_lengths
padding_mask
replace=False)]
abels
eave_unmasked_prob
eters['wps']
cell
needs_sync
gt_sizes
_stdin[0]
candidate_lengths
ilters
dditional_fc
batch["net_input"]["prev_output_tokens"]
len
ref_cnt
shared_encoder_embed_tokens
tgt_dataset
others
fwd_fast_align_cmd
segments_one
optimizers
last_checkpoint
yte_decoder
out_channels
ffset
sentence_loss
collated_samples
pos_embed
attn_scores
ARCH_MODEL_INV_REGISTRY
conv_layer
names
bpe_sentence
block1
pronoun_span
args.decoder_kernel_size_list
flat_hypos
decay_factor
ongest_dataset
odule
moothed_grads_localprev[index]
_mult
ines
full_key
mapped_index
input_feat_per_channel
d_to_strip
finalized_sents
pronoun_with_ws
lement_size
_proj_weight
block_one
lang_pair
clip_coef
escore_score
agg_logging_output[lang_pair]
aragraph
backtranslation_dataset
qa_cat
m_head_transform_weight
wrapped_module
args.distributed_init_method
seq_len
tat
in_len
block2
noisy_src_tokens
pos_num]
best_index
input_args
ias_v
atches['os.path.isfile'].return_value
pieces
high
lprob_z
ias_k
saved_state['prev_value']
vocab_bpe
cand_size
num_lines
hypos[i]
dd_zero_attn
fnames_context
asking_prob
num_mask
eight_dropout
input_options
eters['wall']
word_idx
bitext2_backwards
current_metric
blocks
ctivation_fn
args.encoder_conv_dim
factored
hypo_lst
right_pad_output
num_classes
full_attn_weights
'fairseq-score
counter
lines
bpe_sen
new_item[mask]
checkpoint_conds
src_datasets[lang_pair]
postfix[key]
component_state_dict[component_subkey]
SPACE_NORMALIZER
me_optimizer
in_top1_prob
params_keys
meter
nput_dropout
o_bpe_hypo
etain_dropout
rame_length
gen_output
ds
winners
tsz
ls
h0
stats['loss']
cum_unfin
score_file
ernel_size
'_parser']
toks
xpressivity
outf_label
args.score_dict_dir
nnx_trace
xlm_state_dict
psum
tokens_two
ctx.dim
add_eos_for_other_targets
split_k
chunk
args.log_compression
cpu_buffer
tokens_clone
meta
max_sentences
num_layers
har_inputs
encoder_out['pretrained']['encoder_out']
special
out
dataset_map
re.compile(r'(BLEU4
se_nbm
res
req
orig_target
oken
output_string
agg_output['accuracy']
generate_args.input
end_ds_idx
decoder_embed_tokens
-1]
og_compression
intersect_keys
dictionary
encoder_out['encoder_out']
umulative_sizes
o_bpe_target
convolutions
max_lens
lm_res1_lst
gen_ngrams[bbsz_idx][tuple(ngram[:-1])]
v.size(0)]
args.encoder_learned_pos
keep
stride
length
train_parser
posterior,
forward_output
eters
flat_tgt
non_pad_mask
suffix
conv1d
args.encoder_dropout_out
encoder_out['encoder_padding_mask']
onv_layers
pos_score_dict
valid_subsets
attn_mask
embed_tokens
sentence_target)
lang_pair_dataset_tgt
ka
params_1
params_0
_HDR_MAGIC
qa_list
html_theme_options
icts
sentence_targets
sampling_func
args.self_target
state['optimizer_history']
best_score
uda
thresh
fixed_attention_subset
param_state
pointers
layer_in_channels
state_dict
tensor_size
initial_params
ent_pairs
args.decoder_out_embed_dim
result
clip
__builtin__.print
all_gather_list._buffer
ensorboard_logdir
count_sample_from_first_dataset
usr_parser
args.moses_target_lang
no_bpe_sentences[line_id]
src_off
source_lst
incremental_state
truncated_mask
m_head
args.relu_dropout
si
beams_G
C2.weight
gen_ngrams
start_idx
armup_steps
ordered_targets[gen_keys[key]]
nonpad_idxs
hrink_min
ead
active_mask
ampling_func
ix_lua_indexing
server_tmp
use_fp16
128*128*3
dicts[lang]
oiser
beta2t
nk_idx
url
ax_word_shuffle_distance
curr_updates
exp_avg_sq_col
pos_scores[i]
attention_heads
idirectional
state['extra_state']['train_iterator']
c_out
text
best_idx
score2_file
as_target
feat
overflow
target_prefix_frac_rescore_file
lm_preprocessed_dir
equence_generators[key]
buffered_arange.buf
ontext_window
args.offset
y_b
y_a
cand_bbsz_idx
rerank_data1
rerank_data2
eos_bbsz_idx
forward_input["src_lengths"]
mask_size
c_factor
beam
src_tokens[i]
words
GPU
version_name
residual_dim
rescore_bpe
total_sum
:head_sz]
avg_attn_scores
input_dim
grad
writers[key]
h_txt
is_training
pronoun
args.encoder_glu
data_path
module
w_ind
cur_state
decoder_outs
nsentences
rapped_bar
duplicated_refs
grads_this_group
stats
top_dir
mbed_tokens
args.multihead_attention_nheads
args.tie_adaptive_weights
encoder_out
ight_to_left
state
left_pad_output
unfin_idx
eters['train_loss']
indices_start_line
otal_num_instances
state['optimizer_history'][-1]['optimizer_name']
numChunks
prev_key
sparse_mask
args.decoder_ffn_embed_dim
p32_params
key
key_padding_mask
full_cand
item['target']
loaded_datasets
stats['nll_loss']
backtranslate_datasets[lang_pair]
agg_layers
np_array
num_rand
roject_out_dim
unfold
cf
bias_correction2
tgt_pieces
id_to_src
cs
prev_output_tokens
ct
inal_lr
args.encoder_embed_path
url_or_filename
to_save
full_k
a_end
target[mask]
start:end]
lr
label_dataset
daptive_softmax
isable
ove_eos_to_beginning
output_metadata
retrained
finished
c3
c2
c1
c0
m_output_learned_bias
pct_remaining
ount
hypo_attn
generate_args.max_sentences
state['exp_avg_sq_row']
value
feats
sen_scores
okens_list
ex_str
predictions_bpe_file
globals()[model_name]
vg
refs
rerank_args['weight2']
lassification_heads
os_index
text_spans
tokens_buf
args.encoder_attention_nheads
preprocess_parser
)
reak_mode
nderlying_criterion
state_dict['decoder.version']
Optional[str]
parts
speaker
rref
model_name
active_bbsz_idx
sample_lens
prefix_lprobs
ngram_index
nqueries
cumsum_mask
excl_txt
i
builder
gated_x1
logging_outputs
bad_padding
eters['wpb']
residuals
numFeatures
ecay_steps
last_dim
stats['bsz']
args.non_affine_group_norm
state['extra_state']
rand_or_unmask_prob
ad]
enorm_padding
sources
is_tuple
num_attn_layers
invstddev
torch_cache_home
validate_args
state['_client']
noising_datasets[lang_pair]
caling
logger
active_scores
_stdin_fd
trimed_probs
ample
lr_scheduler
updated_value[key]
parsed_sample
dummy_dict
max_lr
loss2
loss1
joined_file
loss
CONFIG_NAME
ache_index
sizes
finalized_ids
andom_token_prob
token_range
preprocess_param
language_pair_dataset
mbed_out
bitext1_lst
BPE_REGISTRY
utput_dim
len_prefix
postfix['update']
model_params
efault_max_shuffle_distance
export
output_cmvn
tmp
est
gt_dataset
max_epoch
overlap
language_split
breakout
rerank_args[tune_parameters[k]]
len_sen
os_word
d.eos.return_value
total_length
args.weight1
val_key
noise
encoder_out[encoder_padding_mask]
args.distributed_world_size
ps
model_param2
model_param1
embedding.weight.data[idx]
kwargs[arg]
offset
overflows_since_rescale
dict_path
target[i]
eters['ups']
ax_target_positions
state['max_exp_avg_sq']
sys.stdin
word_to_char[i]
n_proj_v
gpus_per_node
chars[eos]
num_lines_in_doc
r_factor
fp32_params.grad
emove_eos_from_tgt
dtype_size
dataset
sen_pos_scores
encoder_input
rads_localprev[index]
grad_norms
sample_id
np_type
block_to_dataset_index
args.decoder_learned_pos
new_item
agg_logging_output
rapped_dict
future_target
real_size
onv_stride
enc_lines
var
num_shards
out_ds["info"]
eam
underlying_ds
args.encoder_dropout_in
pos_score_dict[id_num]
num_pads
hp
search_path
gen_args
ptimizer
buf
files_to_preserve
count
pool_op
j
flat_logging_output['nsentences']
case_k
ayer_norms
random_params
ord_dropout_prob
ttention
odel_lang_pairs
(out_proj)
bigram
rgs_mock.reset_optimizer
result[mask]
int
nested_dataset
state['step']
candidate_tokens
ntasks
generated_sources
ind
sample['net_input']['src_tokens']
bpe_indices
armup_factor
ARCH_MODEL_REGISTRY
rgs.label_smoothing
w1_prob
ep
state['args'].max_target_positions
bool
bidirectional_attention
new_count
args.pretrained
args.encoder
stat
ooler_activation
curr_len
p.grad
backtranslate_datasets
args.decoder_conv_dim
generate_args.buffer_size
prepend_idx
args.conv_dec_config
query_index
input["path"]
first_pronoun_tok
b_size
iter_since_rescale
args.lang_pairs
eos
elf_attention
ymbol_embeddings
ard_selection
ask_idx
eed_reduction
k_layers
lm_loss_sum
